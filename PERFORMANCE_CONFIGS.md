# HalkSpeech Performance Configurations

Bu dosya H100 GPU iÃ§in optimize edilmiÅŸ farklÄ± performans senaryolarÄ±nÄ± iÃ§erir.

## ðŸ“‹ Ä°Ã§indekiler
1. [Maksimum HÄ±z (Turbo Model)](#1-maksimum-hÄ±z-turbo-model)
2. [Maksimum Kalite (Large-v3 Full)](#2-maksimum-kalite-large-v3-full)
3. [Dengeli (Distilled Model)](#3-dengeli-distilled-model)
4. [Model Ä°ndirme](#4-model-iÌ‡ndirme)
5. [Test & Benchmark](#5-test--benchmark)

---

## 1. Maksimum HÄ±z (Turbo Model) ðŸš€

**KullanÄ±m Senaryosu:**
- DÃ¼ÅŸÃ¼k latency kritik (300-500ms)
- 1000'lerce concurrent request
- 20-30 saniyelik kÄ±sa ses kayÄ±tlarÄ±
- HÄ±z > Kalite

**Model:** `deepdml/faster-whisper-large-v3-turbo-ct2`

### Docker Compose

```yaml
version: "3.8"
services:
  halkspeech:
    image: gokay/halkspeach:latest
    container_name: halkspeach
    ports:
      - "8000:8000"
    volumes:
      - /mnt/drive1/models:/home/ubuntu/.cache/huggingface/hub
    environment:
      # Model Settings
      - WHISPER__PRELOAD_MODEL=true

      # Batch Processing (Maksimum HÄ±z)
      - WHISPER__USE_BATCHED_MODE=true
      - WHISPER__BATCH_SIZE=256                      # H100 iÃ§in optimal
      - WHISPER__BATCH_WINDOW_MS=25                  # DÃ¼ÅŸÃ¼k latency (25ms)

      # GPU Settings
      - WHISPER__INFERENCE_DEVICE=cuda
      - WHISPER__DEVICE_INDEX=0
      - WHISPER__COMPUTE_TYPE=float16                # H100 iÃ§in en hÄ±zlÄ±
      - WHISPER__CPU_THREADS=1
      - WHISPER__NUM_WORKERS=1

      # Queue & Concurrency (YÃ¼ksek Throughput)
      - WHISPER__MAX_QUEUE_SIZE=4096                 # Binlerce istek iÃ§in
      - WHISPER__MAX_CONCURRENT_REQUESTS=1000        # AynÄ± anda 1000 istek

      # Model TTL
      - WHISPER__MODEL_TTL=-1                        # Never unload (low latency)

      # Logging
      - LOG_LEVEL=info
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
```

### Docker Run Komutu

```bash
docker run -d --gpus all --name halkspeach \
  -e WHISPER__PRELOAD_MODEL=true \
  -e WHISPER__USE_BATCHED_MODE=true \
  -e WHISPER__BATCH_SIZE=256 \
  -e WHISPER__BATCH_WINDOW_MS=25 \
  -e WHISPER__INFERENCE_DEVICE=cuda \
  -e WHISPER__COMPUTE_TYPE=float16 \
  -e WHISPER__MAX_QUEUE_SIZE=4096 \
  -e WHISPER__MAX_CONCURRENT_REQUESTS=1000 \
  -e WHISPER__MODEL_TTL=-1 \
  -p 8000:8000 \
  -v /mnt/drive1/models:/home/ubuntu/.cache/huggingface/hub \
  gokay/halkspeach:latest
```

### Beklenen Performans

| Metrik | DeÄŸer |
|--------|-------|
| **VRAM KullanÄ±mÄ±** | ~2.5-3GB |
| **Latency (20-30s audio)** | 300-500ms |
| **Throughput** | ~1500-2000 req/min |
| **Word Error Rate (WER)** | ~5-7% (TÃ¼rkÃ§e) |
| **GPU Utilization** | %90-95 |

### Test

```bash
# Model indir
curl "localhost:8000/v1/models/deepdml/faster-whisper-large-v3-turbo-ct2" -X POST

# Test et
time curl -X POST http://localhost:8000/v1/audio/transcriptions \
  -F "file=@test-audio.mp3" \
  -F "model=deepdml/faster-whisper-large-v3-turbo-ct2" \
  -F "response_format=json"
```

---

## 2. Maksimum Kalite (Large-v3 Full) ðŸŽ¯

**KullanÄ±m Senaryosu:**
- Kalite en Ã¶nemli Ã¶ncelik
- Latency 600-800ms kabul edilebilir
- Transkripsiyon doÄŸruluÄŸu kritik
- Kalite > HÄ±z

**Model:** `Systran/faster-whisper-large-v3`

### Docker Compose

```yaml
version: "3.8"
services:
  halkspeech:
    image: gokay/halkspeach:latest
    container_name: halkspeach
    ports:
      - "8000:8000"
    volumes:
      - /mnt/drive1/models:/home/ubuntu/.cache/huggingface/hub
    environment:
      # Model Settings
      - WHISPER__PRELOAD_MODEL=true

      # Batch Processing (Maksimum Kalite)
      - WHISPER__USE_BATCHED_MODE=true
      - WHISPER__BATCH_SIZE=192                      # Biraz dÃ¼ÅŸÃ¼r (model daha bÃ¼yÃ¼k)
      - WHISPER__BATCH_WINDOW_MS=40                  # Daha fazla batch topla

      # GPU Settings
      - WHISPER__INFERENCE_DEVICE=cuda
      - WHISPER__DEVICE_INDEX=0
      - WHISPER__COMPUTE_TYPE=float16                # En iyi kalite
      - WHISPER__CPU_THREADS=1
      - WHISPER__NUM_WORKERS=1

      # Queue & Concurrency (Orta Throughput)
      - WHISPER__MAX_QUEUE_SIZE=2048
      - WHISPER__MAX_CONCURRENT_REQUESTS=600         # Biraz dÃ¼ÅŸÃ¼r

      # Model TTL
      - WHISPER__MODEL_TTL=-1                        # Never unload

      # Logging
      - LOG_LEVEL=info
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
```

### Docker Run Komutu

```bash
docker run -d --gpus all --name halkspeach \
  -e WHISPER__PRELOAD_MODEL=true \
  -e WHISPER__USE_BATCHED_MODE=true \
  -e WHISPER__BATCH_SIZE=192 \
  -e WHISPER__BATCH_WINDOW_MS=40 \
  -e WHISPER__INFERENCE_DEVICE=cuda \
  -e WHISPER__COMPUTE_TYPE=float16 \
  -e WHISPER__MAX_QUEUE_SIZE=2048 \
  -e WHISPER__MAX_CONCURRENT_REQUESTS=600 \
  -e WHISPER__MODEL_TTL=-1 \
  -p 8000:8000 \
  -v /mnt/drive1/models:/home/ubuntu/.cache/huggingface/hub \
  gokay/halkspeach:latest
```

### Beklenen Performans

| Metrik | DeÄŸer |
|--------|-------|
| **VRAM KullanÄ±mÄ±** | ~10-12GB |
| **Latency (20-30s audio)** | 600-800ms |
| **Throughput** | ~1000-1200 req/min |
| **Word Error Rate (WER)** | ~4-5% (TÃ¼rkÃ§e) âœ… (%15-20 daha iyi) |
| **GPU Utilization** | %85-90 |

### Test

```bash
# Model indir (bÃ¼yÃ¼k model, biraz zaman alabilir)
curl "localhost:8000/v1/models/Systran/faster-whisper-large-v3" -X POST

# Test et
time curl -X POST http://localhost:8000/v1/audio/transcriptions \
  -F "file=@test-audio.mp3" \
  -F "model=Systran/faster-whisper-large-v3" \
  -F "response_format=json"
```

---

## 3. Dengeli (Distilled Model) âš–ï¸

**KullanÄ±m Senaryosu:**
- HÄ±z ve kalite dengesi
- Orta latency (400-600ms)
- Ä°yi WER ama hÄ±zlÄ±
- HÄ±z â‰ˆ Kalite

**Model:** `Systran/faster-distil-whisper-large-v3`

### Docker Compose

```yaml
version: "3.8"
services:
  halkspeech:
    image: gokay/halkspeach:latest
    container_name: halkspeach
    ports:
      - "8000:8000"
    volumes:
      - /mnt/drive1/models:/home/ubuntu/.cache/huggingface/hub
    environment:
      # Model Settings
      - WHISPER__PRELOAD_MODEL=true

      # Batch Processing (Dengeli)
      - WHISPER__USE_BATCHED_MODE=true
      - WHISPER__BATCH_SIZE=224                      # OrtasÄ±
      - WHISPER__BATCH_WINDOW_MS=30                  # Dengeli

      # GPU Settings
      - WHISPER__INFERENCE_DEVICE=cuda
      - WHISPER__DEVICE_INDEX=0
      - WHISPER__COMPUTE_TYPE=float16
      - WHISPER__CPU_THREADS=1
      - WHISPER__NUM_WORKERS=1

      # Queue & Concurrency (Dengeli)
      - WHISPER__MAX_QUEUE_SIZE=3072
      - WHISPER__MAX_CONCURRENT_REQUESTS=800

      # Model TTL
      - WHISPER__MODEL_TTL=-1

      # Logging
      - LOG_LEVEL=info
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
```

### Docker Run Komutu

```bash
docker run -d --gpus all --name halkspeach \
  -e WHISPER__PRELOAD_MODEL=true \
  -e WHISPER__USE_BATCHED_MODE=true \
  -e WHISPER__BATCH_SIZE=224 \
  -e WHISPER__BATCH_WINDOW_MS=30 \
  -e WHISPER__INFERENCE_DEVICE=cuda \
  -e WHISPER__COMPUTE_TYPE=float16 \
  -e WHISPER__MAX_QUEUE_SIZE=3072 \
  -e WHISPER__MAX_CONCURRENT_REQUESTS=800 \
  -e WHISPER__MODEL_TTL=-1 \
  -p 8000:8000 \
  -v /mnt/drive1/models:/home/ubuntu/.cache/huggingface/hub \
  gokay/halkspeach:latest
```

### Beklenen Performans

| Metrik | DeÄŸer |
|--------|-------|
| **VRAM KullanÄ±mÄ±** | ~5-6GB |
| **Latency (20-30s audio)** | 400-600ms |
| **Throughput** | ~1300-1500 req/min |
| **Word Error Rate (WER)** | ~5-6% (TÃ¼rkÃ§e) |
| **GPU Utilization** | %90-93 |

---

## 4. Model Ä°ndirme

### TÃ¼m Modelleri Ä°ndir

```bash
# Turbo (HÄ±z)
curl "localhost:8000/v1/models/deepdml/faster-whisper-large-v3-turbo-ct2" -X POST

# Large-v3 Full (Kalite)
curl "localhost:8000/v1/models/Systran/faster-whisper-large-v3" -X POST

# Distilled (Dengeli)
curl "localhost:8000/v1/models/Systran/faster-distil-whisper-large-v3" -X POST
```

### Ä°ndirme SÃ¼resi (H100 + hÄ±zlÄ± internet)

| Model | Boyut | Ä°ndirme SÃ¼resi |
|-------|-------|----------------|
| Turbo | ~1.5GB | ~1-2 dakika |
| Distilled | ~2.5GB | ~2-3 dakika |
| Large-v3 Full | ~3GB | ~3-4 dakika |

---

## 5. Test & Benchmark

### Latency Test (Tek Ä°stek)

```bash
# Her model iÃ§in latency Ã¶lÃ§
for model in \
  "deepdml/faster-whisper-large-v3-turbo-ct2" \
  "Systran/faster-distil-whisper-large-v3" \
  "Systran/faster-whisper-large-v3"
do
  echo "=== Testing $model ==="
  time curl -s -X POST http://localhost:8000/v1/audio/transcriptions \
    -F "file=@test-audio.mp3" \
    -F "model=$model" \
    -F "response_format=json" | jq -r '.text' | head -c 100
  echo -e "\n"
done
```

### Throughput Test (Apache Bench)

```bash
# 1000 istek, 100 concurrent
ab -n 1000 -c 100 \
  -p test-audio.mp3 \
  -T "audio/mpeg" \
  "http://localhost:8000/v1/audio/transcriptions?model=deepdml/faster-whisper-large-v3-turbo-ct2"
```

### GPU Monitoring

```bash
# GPU kullanÄ±mÄ±nÄ± izle
watch -n 1 nvidia-smi

# DetaylÄ± metrics
nvidia-smi dmon -s pucvmet -d 1
```

### Docker Logs

```bash
# Real-time logs
docker logs -f halkspeach

# Son 100 satÄ±r
docker logs --tail 100 halkspeach

# Sadece hatalar
docker logs halkspeach 2>&1 | grep ERROR
```

---

## ðŸ“Š Model KarÅŸÄ±laÅŸtÄ±rma Tablosu

| Ã–zellik | Turbo (HÄ±z) | Distilled (Dengeli) | Large-v3 (Kalite) |
|---------|-------------|---------------------|-------------------|
| **Model** | deepdml/turbo | Systran/distilled | Systran/large-v3 |
| **VRAM** | 2.5GB | 5-6GB | 10-12GB |
| **Latency** | 300-500ms âœ… | 400-600ms | 600-800ms |
| **Throughput** | 2000/min âœ… | 1500/min | 1200/min |
| **WER (TÃ¼rkÃ§e)** | ~5-7% | ~5-6% | ~4-5% âœ… |
| **Batch Size** | 256 | 224 | 192 |
| **Batch Window** | 25ms | 30ms | 40ms |
| **Max Requests** | 1000 | 800 | 600 |
| **Use Case** | HÄ±z kritik | Dengeli | Kalite kritik |

---

## âš™ï¸ DiÄŸer Optimizasyon SeÃ§enekleri

### Ultra DÃ¼ÅŸÃ¼k Latency (Acil Durumlar)

```bash
# Batch window'u minimuma indir
WHISPER__BATCH_WINDOW_MS=10

# Batch size'Ä± dÃ¼ÅŸÃ¼r
WHISPER__BATCH_SIZE=128
```

### Ultra YÃ¼ksek Throughput (Latency Ã¶nemsiz)

```bash
# Batch window'u artÄ±r
WHISPER__BATCH_WINDOW_MS=100

# Batch size'Ä± artÄ±r
WHISPER__BATCH_SIZE=384

# Queue'yu artÄ±r
WHISPER__MAX_QUEUE_SIZE=8192
```

### VRAM Tasarrufu (Quantization)

```bash
# int8 quantization kullan (~50% VRAM tasarrufu)
WHISPER__COMPUTE_TYPE=int8_float16

# Batch size'Ä± dÃ¼ÅŸÃ¼r
WHISPER__BATCH_SIZE=128
```

---

## ðŸŽ¯ HÄ±zlÄ± BaÅŸlangÄ±Ã§

### Senaryo 1: "HÄ±z lazÄ±m, kalite yeter"
```bash
# Turbo model kullan (yukarÄ±daki 1. config)
docker run ... -e WHISPER__BATCH_SIZE=256 ...
```

### Senaryo 2: "Kalite Ã§ok Ã¶nemli"
```bash
# Large-v3 Full kullan (yukarÄ±daki 2. config)
docker run ... -e WHISPER__BATCH_SIZE=192 ...
```

### Senaryo 3: "OrtasÄ±nÄ± bul"
```bash
# Distilled model kullan (yukarÄ±daki 3. config)
docker run ... -e WHISPER__BATCH_SIZE=224 ...
```

---

## ðŸ“ Notlar

### âš ï¸ Ã–nemli UyarÄ±lar

1. **SAMPLES_PER_SECOND asla deÄŸiÅŸtirme!**
   - Whisper sadece 16kHz iÃ§in eÄŸitilmiÅŸ
   - Bu deÄŸer deÄŸiÅŸirse transkripsiyonlar bozulur

2. **Batch size Ã§ok yÃ¼ksek â†’ OOM (Out of Memory)**
   - H100 80GB iÃ§in max ~384-512 (model boyutuna gÃ¶re)
   - Ä°lk baÅŸta Ã¶nerilen deÄŸerlerle baÅŸla

3. **Model preload Ã¶neriliir**
   - `WHISPER__PRELOAD_MODEL=true` kullan
   - Ä°lk istek daha hÄ±zlÄ± gelir

4. **VAD filter otomatik aktif**
   - BatchedInferencePipeline iÃ§in otomatik `vad_filter=True`
   - BÃ¼yÃ¼k dosyalar iÃ§in gerekli

### ðŸ”§ Troubleshooting

**Problem:** GPU kullanÄ±mÄ± dÃ¼ÅŸÃ¼k (%30-40)
```bash
# Batch size ve concurrent requests artÄ±r
WHISPER__BATCH_SIZE=320
WHISPER__MAX_CONCURRENT_REQUESTS=1200
```

**Problem:** Latency Ã§ok yÃ¼ksek (>1s)
```bash
# Batch window dÃ¼ÅŸÃ¼r, batch size dÃ¼ÅŸÃ¼r
WHISPER__BATCH_WINDOW_MS=15
WHISPER__BATCH_SIZE=128
```

**Problem:** OOM (Out of Memory) hatasÄ±
```bash
# Batch size dÃ¼ÅŸÃ¼r veya quantization kullan
WHISPER__BATCH_SIZE=128
WHISPER__COMPUTE_TYPE=int8_float16
```

---

## ðŸ“š Ek Kaynaklar

- [faster-whisper GitHub](https://github.com/SYSTRAN/faster-whisper)
- [Whisper Model Card](https://huggingface.co/openai/whisper-large-v3)
- [HalkSpeech Docs](https://github.com/GkyEla/halkspeech)

---

**Son GÃ¼ncelleme:** 2025-11-14
**GPU:** NVIDIA H100 80GB
**Docker Image:** gokay/halkspeach:latest
