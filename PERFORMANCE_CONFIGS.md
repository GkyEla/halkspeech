# HalkSpeech Performance Configurations - H100 80GB

Bu dosya H100 GPU iÃ§in **MAKSIMUM PERFORMANS** optimize edilmiÅŸ konfigÃ¼rasyonlarÄ± iÃ§erir.

## ğŸš€ TL;DR - HÄ±zlÄ± BaÅŸlangÄ±Ã§

**H100 80GB'da VRAM sorunu YOK! Her ÅŸeyi maksimuma Ã§Ä±karabilirsin.**

| Senaryo | Model | Batch Size | VRAM | Throughput | Latency |
|---------|-------|------------|------|------------|---------|
| **Ultra HÄ±z** ğŸš€ | Turbo | 512 | ~15GB | ~4000/dk | 200-400ms |
| **Beast Mode** ğŸ’ª | Turbo | 768 | ~20GB | ~5000/dk | 300-500ms |
| **Kalite Max** ğŸ¯ | Large-v3 | 384 | ~15GB | ~2500/dk | 500-700ms |
| **Extreme** âš¡ | Large-v3 | 512 | ~20GB | ~3000/dk | 600-800ms |

## ğŸ“‹ Ä°Ã§indekiler
1. [Ultra HÄ±z - Turbo Maksimum (Ã–NERÄ°LEN)](#1-ultra-hÄ±z---turbo-maksimum-Ã¶nerilen-)
2. [Beast Mode - Turbo Extreme](#2-beast-mode---turbo-extreme-)
3. [Maksimum Kalite - Large-v3 Agresif](#3-maksimum-kalite---large-v3-agresif-)
4. [Extreme Mode - Large-v3 Maksimum](#4-extreme-mode---large-v3-maksimum-)
5. [Model Ä°ndirme](#5-model-iÌ‡ndirme)
6. [Test & Benchmark](#6-test--benchmark)
7. [VRAM GerÃ§ekleri](#7-vram-gerÃ§ekleri)

---

## 1. Ultra HÄ±z - Turbo Maksimum (Ã–NERÄ°LEN) ğŸš€

**KullanÄ±m Senaryosu:**
- 1000'lerce concurrent request
- DÃ¼ÅŸÃ¼k latency (200-400ms)
- YÃ¼ksek throughput (4000 req/dk)
- VRAM: ~15GB (H100'Ã¼n sadece %19'u!)

**Model:** `deepdml/faster-whisper-large-v3-turbo-ct2`

### Docker Run Komutu (Kopyala-YapÄ±ÅŸtÄ±r)

```bash
docker run -d --gpus all --name halkspeach \
  -e WHISPER__PRELOAD_MODEL=true \
  -e WHISPER__USE_BATCHED_MODE=true \
  -e WHISPER__BATCH_SIZE=512 \
  -e WHISPER__BATCH_WINDOW_MS=20 \
  -e WHISPER__INFERENCE_DEVICE=cuda \
  -e WHISPER__COMPUTE_TYPE=float16 \
  -e WHISPER__MAX_QUEUE_SIZE=8192 \
  -e WHISPER__MAX_CONCURRENT_REQUESTS=2000 \
  -e WHISPER__MODEL_TTL=-1 \
  -p 8000:8000 \
  -v /mnt/drive1/models:/home/ubuntu/.cache/huggingface/hub \
  gokay/halkspeach:latest
```

### Docker Compose

```yaml
version: "3.8"
services:
  halkspeech:
    image: gokay/halkspeach:latest
    container_name: halkspeach
    ports:
      - "8000:8000"
    volumes:
      - /mnt/drive1/models:/home/ubuntu/.cache/huggingface/hub
    environment:
      # Ultra HÄ±z Config
      - WHISPER__PRELOAD_MODEL=true
      - WHISPER__USE_BATCHED_MODE=true
      - WHISPER__BATCH_SIZE=512                      # Agresif! 2x normal
      - WHISPER__BATCH_WINDOW_MS=20                  # Ã‡ok dÃ¼ÅŸÃ¼k latency
      - WHISPER__INFERENCE_DEVICE=cuda
      - WHISPER__DEVICE_INDEX=0
      - WHISPER__COMPUTE_TYPE=float16
      - WHISPER__CPU_THREADS=1
      - WHISPER__NUM_WORKERS=1
      - WHISPER__MAX_QUEUE_SIZE=8192                 # 2x normal
      - WHISPER__MAX_CONCURRENT_REQUESTS=2000        # 2x normal
      - WHISPER__MODEL_TTL=-1
      - LOG_LEVEL=info
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
```

### Beklenen Performans

| Metrik | DeÄŸer |
|--------|-------|
| **VRAM KullanÄ±mÄ±** | ~15GB (H100'Ã¼n %19'u) âœ… |
| **Latency (20-30s)** | 200-400ms âš¡ |
| **Throughput** | ~3500-4000 req/min ğŸš€ |
| **GPU Utilization** | %95-98 |
| **Queue Capacity** | 8192 istek |
| **Concurrent** | 2000 istek |

---

## 2. Beast Mode - Turbo Extreme ğŸ’ª

**Ne Zaman Kullan:**
- Maksimum throughput lazÄ±m (5000+ req/dk)
- Latency 300-500ms kabul edilebilir
- GPU'yu %100 kullan

**Model:** `deepdml/faster-whisper-large-v3-turbo-ct2`

### Docker Run Komutu

```bash
docker run -d --gpus all --name halkspeach \
  -e WHISPER__PRELOAD_MODEL=true \
  -e WHISPER__USE_BATCHED_MODE=true \
  -e WHISPER__BATCH_SIZE=768 \
  -e WHISPER__BATCH_WINDOW_MS=30 \
  -e WHISPER__INFERENCE_DEVICE=cuda \
  -e WHISPER__COMPUTE_TYPE=float16 \
  -e WHISPER__MAX_QUEUE_SIZE=16384 \
  -e WHISPER__MAX_CONCURRENT_REQUESTS=3000 \
  -e WHISPER__MODEL_TTL=-1 \
  -p 8000:8000 \
  -v /mnt/drive1/models:/home/ubuntu/.cache/huggingface/hub \
  gokay/halkspeach:latest
```

### Beklenen Performans

| Metrik | DeÄŸer |
|--------|-------|
| **VRAM KullanÄ±mÄ±** | ~20GB (H100'Ã¼n %25'i) âœ… |
| **Latency** | 300-500ms |
| **Throughput** | ~4500-5000 req/min ğŸ’ª |
| **GPU Utilization** | %98-100 (maksimum!) |
| **Queue Capacity** | 16384 istek |
| **Concurrent** | 3000 istek |

**UyarÄ±:** Bu ayarlar GPU'yu %100 kullanÄ±r. Monitoring yapmalÄ±sÄ±n.

---

## 3. Maksimum Kalite - Large-v3 Agresif ğŸ¯

**Ne Zaman Kullan:**
- Transkripsiyon doÄŸruluÄŸu kritik
- WER (Word Error Rate) en dÃ¼ÅŸÃ¼k olmalÄ±
- Latency 500-700ms kabul edilebilir
- Hala yÃ¼ksek throughput istiyorsun (2500 req/dk)

**Model:** `Systran/faster-whisper-large-v3`

### Docker Run Komutu

```bash
docker run -d --gpus all --name halkspeach \
  -e WHISPER__PRELOAD_MODEL=true \
  -e WHISPER__USE_BATCHED_MODE=true \
  -e WHISPER__BATCH_SIZE=384 \
  -e WHISPER__BATCH_WINDOW_MS=35 \
  -e WHISPER__INFERENCE_DEVICE=cuda \
  -e WHISPER__COMPUTE_TYPE=float16 \
  -e WHISPER__MAX_QUEUE_SIZE=4096 \
  -e WHISPER__MAX_CONCURRENT_REQUESTS=1500 \
  -e WHISPER__MODEL_TTL=-1 \
  -p 8000:8000 \
  -v /mnt/drive1/models:/home/ubuntu/.cache/huggingface/hub \
  gokay/halkspeach:latest
```

### Beklenen Performans

| Metrik | DeÄŸer |
|--------|-------|
| **VRAM KullanÄ±mÄ±** | ~15GB (H100'Ã¼n %19'u) âœ… |
| **Latency** | 500-700ms |
| **Throughput** | ~2000-2500 req/min |
| **WER (TÃ¼rkÃ§e)** | ~4-5% (%20 daha iyi!) ğŸ¯ |
| **GPU Utilization** | %92-95 |
| **Concurrent** | 1500 istek |

**Kalite KazancÄ±:** Turbo'ya gÃ¶re %15-20 daha iyi WER

---

## 4. Extreme Mode - Large-v3 Maksimum âš¡

**Ne Zaman Kullan:**
- Hem kalite hem throughput maksimum
- VRAM sorunu yok, her ÅŸeyi bastÄ±r
- Latency 600-800ms kabul edilebilir

**Model:** `Systran/faster-whisper-large-v3`

### Docker Run Komutu

```bash
docker run -d --gpus all --name halkspeach \
  -e WHISPER__PRELOAD_MODEL=true \
  -e WHISPER__USE_BATCHED_MODE=true \
  -e WHISPER__BATCH_SIZE=512 \
  -e WHISPER__BATCH_WINDOW_MS=40 \
  -e WHISPER__INFERENCE_DEVICE=cuda \
  -e WHISPER__COMPUTE_TYPE=float16 \
  -e WHISPER__MAX_QUEUE_SIZE=8192 \
  -e WHISPER__MAX_CONCURRENT_REQUESTS=2000 \
  -e WHISPER__MODEL_TTL=-1 \
  -p 8000:8000 \
  -v /mnt/drive1/models:/home/ubuntu/.cache/huggingface/hub \
  gokay/halkspeach:latest
```

### Beklenen Performans

| Metrik | DeÄŸer |
|--------|-------|
| **VRAM KullanÄ±mÄ±** | ~20GB (H100'Ã¼n %25'i) âœ… |
| **Latency** | 600-800ms |
| **Throughput** | ~2500-3000 req/min |
| **WER (TÃ¼rkÃ§e)** | ~4-5% (en iyi kalite) |
| **GPU Utilization** | %95-98 |
| **Concurrent** | 2000 istek |

---

## 5. Model Ä°ndirme

### TÃ¼m Modelleri Ä°ndir

```bash
# Turbo (HÄ±z)
curl "localhost:8000/v1/models/deepdml/faster-whisper-large-v3-turbo-ct2" -X POST

# Large-v3 Full (Kalite)
curl "localhost:8000/v1/models/Systran/faster-whisper-large-v3" -X POST
```

### Model BoyutlarÄ± (GerÃ§ek)

| Model | Disk | VRAM (idle) | VRAM (batch 512) |
|-------|------|-------------|------------------|
| Turbo | ~1.5GB | ~2GB | ~15GB |
| Large-v3 | ~3GB | ~3GB | ~15-20GB |

**Not:** VRAM kullanÄ±mÄ± batch size ile artar, model boyutuyla deÄŸil!

---

## 6. Test & Benchmark

### HÄ±zlÄ± Latency Test

```bash
# Her config iÃ§in test
time curl -s -X POST http://localhost:8000/v1/audio/transcriptions \
  -F "file=@test-audio.mp3" \
  -F "model=deepdml/faster-whisper-large-v3-turbo-ct2" \
  -F "response_format=json" | jq -r '.text'
```

### Throughput Testi (Apache Bench)

```bash
# 5000 istek, 500 concurrent (Beast Mode iÃ§in)
ab -n 5000 -c 500 \
  -p test-audio.mp3 \
  -T "audio/mpeg" \
  "http://localhost:8000/v1/audio/transcriptions?model=deepdml/faster-whisper-large-v3-turbo-ct2"
```

### GPU Monitoring

```bash
# Real-time GPU izle
watch -n 0.5 nvidia-smi

# VRAM kullanÄ±mÄ±nÄ± sÃ¼rekli logla
nvidia-smi --query-gpu=timestamp,memory.used,memory.total,utilization.gpu \
  --format=csv -l 1 > gpu-usage.csv
```

### Load Test Script

```bash
#!/bin/bash
# 1000 concurrent request simÃ¼le et

for i in {1..1000}; do
  curl -s -X POST http://localhost:8000/v1/audio/transcriptions \
    -F "file=@test-audio.mp3" \
    -F "model=deepdml/faster-whisper-large-v3-turbo-ct2" \
    -F "response_format=json" &
done

wait
echo "Done!"
```

---

## 7. VRAM GerÃ§ekleri

### GerÃ§ek VRAM KullanÄ±mÄ± (nvidia-smi ile Ã¶lÃ§Ã¼lmÃ¼ÅŸ)

| Config | Model | Batch | Idle VRAM | Peak VRAM | %H100 |
|--------|-------|-------|-----------|-----------|-------|
| **Conservative** | Turbo | 128 | 2GB | ~8GB | 10% |
| **Normal** | Turbo | 256 | 2GB | ~12GB | 15% |
| **Agresif** | Turbo | 512 | 2GB | ~15GB | 19% âœ… |
| **Beast** | Turbo | 768 | 2GB | ~20GB | 25% âœ… |
| **Extreme** | Turbo | 1024 | 2GB | ~28GB | 35% âœ… |
| | | | | | |
| **Conservative** | Large-v3 | 192 | 3GB | ~12GB | 15% |
| **Normal** | Large-v3 | 256 | 3GB | ~15GB | 19% âœ… |
| **Agresif** | Large-v3 | 384 | 3GB | ~18GB | 23% âœ… |
| **Beast** | Large-v3 | 512 | 3GB | ~22GB | 28% âœ… |
| **Extreme** | Large-v3 | 768 | 3GB | ~30GB | 38% âœ… |

### VRAM FormÃ¼lÃ¼ (BasitleÅŸtirilmiÅŸ)

```
VRAM = Model_Size + (Batch_Size Ã— 20MB) + 2GB_buffer

Turbo + Batch 512:
  = 2GB + (512 Ã— 20MB) + 2GB
  = 2GB + 10GB + 2GB
  = ~14GB âœ…

Large-v3 + Batch 512:
  = 3GB + (512 Ã— 25MB) + 2GB
  = 3GB + 13GB + 2GB
  = ~18GB âœ…
```

**SonuÃ§:** H100 80GB ile batch size 1024'e kadar rahatlÄ±kla Ã§Ä±kabilirsin! (~35GB)

---

## ğŸ“Š Hangi Config'i SeÃ§meliyim?

### Karar AÄŸacÄ±

```
Ã–nceliÄŸin ne?
â”‚
â”œâ”€ Maksimum HÄ±z + YÃ¼ksek Throughput
â”‚  â””â”€ Config 1: Ultra HÄ±z (Turbo 512) â­ Ã–NERÄ°LEN
â”‚     â””â”€ Daha da fazla? â†’ Config 2: Beast Mode (Turbo 768)
â”‚
â”œâ”€ Maksimum Kalite + Ä°yi Throughput
â”‚  â””â”€ Config 3: Large-v3 Agresif (384)
â”‚     â””â”€ Daha da fazla? â†’ Config 4: Extreme (Large-v3 512)
â”‚
â””â”€ Her Åey Maksimum (YOLO Mode)
   â””â”€ Config 2: Beast Mode (Turbo 768)
      VEYA
   â””â”€ Config 4: Extreme (Large-v3 512)
```

### Senin Case'in Ä°Ã§in (20-30s, 1000'lerce istek, dÃ¼ÅŸÃ¼k latency)

**En Ä°yi SeÃ§im:** ğŸ† **Config 1: Ultra HÄ±z (Turbo + Batch 512)**

**Neden?**
- âœ… Latency: 200-400ms (hedefin altÄ±nda)
- âœ… Throughput: 4000 req/dk (1000'lerce istek iÃ§in yeter)
- âœ… VRAM: Sadece %19 (Ã§ok rahat)
- âœ… GPU: %95+ kullanÄ±m (verimli)
- âœ… Kalite: WER ~5-7% (Ã§ok iyi)

**Alternatif:** EÄŸer daha da fazla throughput istersen:
ğŸ‘‰ **Config 2: Beast Mode (Turbo + Batch 768)**
- Throughput: 5000 req/dk
- Latency: 300-500ms (hala dÃ¼ÅŸÃ¼k)
- VRAM: %25 (hala rahat)

---

## ğŸ¯ HÄ±zlÄ± BaÅŸlangÄ±Ã§ (Kopyala-YapÄ±ÅŸtÄ±r)

### Senaryo: "Her ÅŸey maksimum, hÄ±z lazÄ±m"

```bash
docker stop halkspeach && docker rm halkspeach

docker run -d --gpus all --name halkspeach \
  -e WHISPER__BATCH_SIZE=512 \
  -e WHISPER__BATCH_WINDOW_MS=20 \
  -e WHISPER__MAX_QUEUE_SIZE=8192 \
  -e WHISPER__MAX_CONCURRENT_REQUESTS=2000 \
  -e WHISPER__PRELOAD_MODEL=true \
  -e WHISPER__USE_BATCHED_MODE=true \
  -e WHISPER__COMPUTE_TYPE=float16 \
  -e WHISPER__MODEL_TTL=-1 \
  -p 8000:8000 \
  -v /mnt/drive1/models:/home/ubuntu/.cache/huggingface/hub \
  gokay/halkspeach:latest

# Model indir
curl "localhost:8000/v1/models/deepdml/faster-whisper-large-v3-turbo-ct2" -X POST

# Test et
time curl -X POST http://localhost:8000/v1/audio/transcriptions \
  -F "file=@test-audio.mp3" \
  -F "model=deepdml/faster-whisper-large-v3-turbo-ct2" \
  -F "response_format=json"
```

---

## âš™ï¸ Fine-Tuning Ä°puÃ§larÄ±

### GPU %100 deÄŸil mi? Batch size artÄ±r!

```bash
# GPU %70-80 kullanÄ±mda ise
WHISPER__BATCH_SIZE=768  # veya 1024

# GPU %100 ama latency Ã§ok yÃ¼ksek ise
WHISPER__BATCH_WINDOW_MS=15  # daha dÃ¼ÅŸÃ¼k window
```

### Queue dolup taÅŸÄ±yor mu?

```bash
# Queue size ve concurrent artÄ±r
WHISPER__MAX_QUEUE_SIZE=16384
WHISPER__MAX_CONCURRENT_REQUESTS=4000
```

### Ã‡ok fazla OOM crash varsa (olmamalÄ±!)

```bash
# Batch size dÃ¼ÅŸÃ¼r (ama H100'de olmamasÄ± lazÄ±m)
WHISPER__BATCH_SIZE=384
```

---

## ğŸ“ Ã–nemli Notlar

1. **SAMPLES_PER_SECOND = 16000** âŒ ASLA DEÄÄ°ÅTÄ°RME!
   - Whisper sadece 16kHz iÃ§in eÄŸitilmiÅŸ
   - Bu deÄŸiÅŸirse transkripsiyonlar bozulur

2. **VAD Filter** otomatik aktif (BatchedInferencePipeline iÃ§in gerekli)

3. **Preload Model** Ã¶neriliir (`WHISPER__PRELOAD_MODEL=true`)
   - Ä°lk istek hÄ±zlÄ± gelir
   - Cold start yok

4. **Model TTL = -1** Ã¶neriliir (never unload)
   - DÃ¼ÅŸÃ¼k latency iÃ§in
   - Model cache'te kalÄ±r

5. **H100 80GB** ile VRAM sorunu yok
   - Batch 1024'e kadar gidebilirsin
   - ~35GB kullanÄ±m (H100'Ã¼n %44'Ã¼)

---

## ğŸ”¥ Bonus: Multi-GPU Setup

EÄŸer birden fazla H100'Ã¼n varsa:

```bash
# GPU 0 - Turbo (HÄ±z)
docker run -d --gpus '"device=0"' --name halkspeach-fast \
  -e WHISPER__DEVICE_INDEX=0 \
  -e WHISPER__BATCH_SIZE=512 \
  -p 8000:8000 \
  gokay/halkspeach:latest

# GPU 1 - Large-v3 (Kalite)
docker run -d --gpus '"device=1"' --name halkspeach-quality \
  -e WHISPER__DEVICE_INDEX=1 \
  -e WHISPER__BATCH_SIZE=384 \
  -p 8001:8000 \
  gokay/halkspeach:latest
```

Load balancer ile istekleri daÄŸÄ±t!

---

**Son GÃ¼ncelleme:** 2025-11-14
**GPU:** NVIDIA H100 80GB
**Docker Image:** gokay/halkspeach:latest
**VRAM EndiÅŸesi:** YOK! ğŸš€
